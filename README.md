# Semi-supervised learning GAN in Tensorflow

This is my [Tensorflow](https://www.tensorflow.org/) implementation of 
**Semi-supervised Learning Generative Adversarial Networks** proposed in the paper
[Improved Techniques for Training GANs](http://arxiv.org/abs/1606.03498).

Similar work incldues 
[Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks](https://arxiv.org/abs/1511.06390), 
[Semi-Supervised Learning with Generative Adversarial Networks](https://arxiv.org/abs/1606.01583), etc.
The main idea of this line of work is exploiting the samples generated by GAN generators to boost the performance of image classification tasks by improving generalization.

## Prerequisites

- Python 2.7 or Python 3.3+
- [Tensorflow 1.0.0](https://github.com/tensorflow/tensorflow/tree/r1.0)
- [SciPy](http://www.scipy.org/install.html)
- [NumPy](http://www.numpy.org/)

## Usage

Download datasets with:

    $ python download.py mnist

To train a model with downloaded dataset:

    $ python trainer.py --dataset mnist

To test with an existing model:

    $ python eval.py --dataset mnist --checkpoint ckpt_dir

Or, you can use your own dataset by:

    $ mkdir data/YOUR_DATASET
    ... add images to data/DATASET_NAME ...
    $ python trainer.py --dataset YOUR_DATASET --input_height h --input_width w
    $ python evaler.py --dataset YOUR_DATASET --input_height h --input_width w

## Results
<!--
![result](assets/training.gif)
-->
### MNIST
<!--
MNIST codes are written by [@PhoenixDai](https://github.com/PhoenixDai).

![mnist_result1](assets/mnist1.png)

![mnist_result2](assets/mnist2.png)

![mnist_result3](assets/mnist3.png)
-->
More results can be found [here](./assets/) and [here](./web/img/).

## Training details

Details of the loss of Discriminator and Generator (MNIST dataset).
<!--
![d_loss](assets/d_loss.png)

![g_loss](assets/g_loss.png)
-->
Details of the histogram of true and fake result of discriminator (MNIST dataset).
<!--
![d_hist](assets/d_hist.png)

![d__hist](assets/d__hist.png)
-->

## Training tricks

To avoid the fast convergence of D (discriminator) network, G (generator) network is updated twice for each D network update, which differs from original paper.

## Author

Shao-Hua Sun / [@shaohua0116](http://shaohua0116.github.io/)
